<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>sma</title>
</head>

<body>
<pre>
    1.Content analysis: Analyze the content of social media data to determine what 
    topics are being given data set using topic modelling, keyword extractor
    '''
    This program first loads the social media data from a CSV file and removes any rows with missing values. It then uses the langdetect library to remove any non-English content, and performs text preprocessing by removing stopwords and performing lemmatization using the NLTK library.
    
    Next, it creates a document-term matrix using TF-IDF vectorization, and converts it to a gensim corpus. It then uses the LdaModel class from gensim to perform LDA topic modeling on the corpus, specifying the number of topics to be 5.
    
    Finally, it visualizes the topics using the pyLDAvis library, which generates an interactive visualization of the topics. You can then use this visualization to explore the topics and their associated words and documents.
    
    '''
    
    from nltk.stem import WordNetLemmatizer
    from nltk.corpus import stopwords
    import csv
    import numpy as np
    import pandas as pd
    from langdetect import detect
    from textblob import TextBlob
    from sklearn.feature_extraction.text import TfidfVectorizer
    from gensim import corpora, models
    import pyLDAvis.gensim
    
    # Load social media data from a CSV file
    data = pd.read_csv("social_media_data.csv")
    
    # Remove any rows with missing values
    data = data.dropna()
    
    # Remove any non-English content
    data['language'] = data['text'].apply(lambda x: detect(x))
    data = data[data['language'] == 'en']
    
    # Clean up the text by removing stopwords and performing lemmatization
    stop_words = stopwords.words('english')
    lemmatizer = WordNetLemmatizer()
    
    
    def preprocess(text):
        words = TextBlob(text).words.singularize()
        words = [lemmatizer.lemmatize(word)
                 for word in words if word not in stop_words]
        return ' '.join(words)
    
    
    data['clean_text'] = data['text'].apply(preprocess)
    
    # Create a document-term matrix using TF-IDF vectorization
    vectorizer = TfidfVectorizer()
    doc_term_matrix = vectorizer.fit_transform(data['clean_text'])
    
    # Convert the document-term matrix to a gensim corpus
    corpus = corpora.MmCorpus(doc_term_matrix.transpose())
    
    # Perform LDA topic modeling on the corpus
    lda_model = models.LdaModel(
        corpus=corpus, num_topics=5, id2word=vectorizer.get_feature_names())
    
    # Visualize the topics using pyLDAvis
    pyLDAvis.enable_notebook()
    vis = pyLDAvis.gensim.prepare(lda_model, corpus, vectorizer)
    pyLDAvis.display(vis)
    
    '''
    The output of the above code would be an interactive visualization of the topics generated by the LDA model using the pyLDAvis library. The visualization would show the following:
    
    A two-dimensional plot of the topics, where each circle represents a topic and the size of the circle represents the prevalence of the topic in the corpus.
    A list of the top 30 most relevant terms for each topic, where relevance is calculated as a combination of the term frequency and the exclusivity of the term to the topic.
    A horizontal bar chart that shows the frequency of each term in the corpus, along with its overall frequency across all topics.
    '''
    
    ----------------
    1.1 Content analysis: Analyze the content of social media data to determine what 
    topics are being given data set using topic modelling, keyword extractor
    import pandas as pd
    
    # !pip install pyLDAvis
    import pyLDAvis.gensim_models
    
    from gensim import corpora
    from gensim.models import LdaModel
    from gensim.utils import simple_preprocess
    from gensim.parsing.preprocessing import STOPWORDS
    
    df = pd.read_csv('social_media_data.csv')
    
    stop_words = set(STOPWORDS)
    def preprocess(text):
        result = []
        for token in simple_preprocess(text):
            if token not in stop_words:
                result.append(token)
    
        return result
    
    
    texts = df['text'].apply(preprocess)
    dictionary = corpora.Dictionary(texts)
    corpus = [dictionary.doc2bow(text) for text in texts]
    
    num_topics = 5
    passes = 10
    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=passes)
    
    for topic in lda_model.print_topics():
        print(topic)
    
    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)
    pyLDAvis.display(vis)
    ------------------------------------------------------------------------------------------------------------------------------
    2.Location analysis: Analyze the location data associated with tweets to understand 
    where particular location are most prevalent.
    
    # Exp 2
    '''
    This program first loads the tweet data from a CSV file and removes any rows with missing values. It then creates a new column for the state abbreviation based on the location data by using a dictionary of state abbreviations and their full names, and a function that searches for these abbreviations in the location data.
    
    Next, it plots the frequency of each state in the data using the Seaborn library. It creates a bar plot of the state frequencies using the state_counts DataFrame, and customizes the plot with labels and titles. The resulting plot shows the relative frequency of each state in the tweet data, which can help to identify where particular locations are most prevalent.
    '''
    
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # Load the tweet data from a CSV file
    data = pd.read_csv("/content/tweet_data.csv")
    
    # Remove any rows with missing values
    data = data.dropna()
    
    # Create a new column for the state abbreviation based on the location data
    states = {
        'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',
        'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',
        'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',
        'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',
        'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',
        'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',
        'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',
        'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',
        'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',
        'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'
    }
    
    
    def get_state(location):
        words = location.split()
        for word in words:
            if word.upper() in states:
                return states[word.upper()]
        return None
    
    
    data['state'] = data['location'].apply(get_state)
    print(data.head())
    
    # Plot the frequency of each state in the data
    state_counts = data['state'].value_counts()
    # print("Count", state_counts)
    plt.figure(figsize=(12, 6))
    
    # name, values
    sns.barplot(x=state_counts.index, y=state_counts.values, palette="rocket")
    plt.xticks(rotation=90)
    plt.xlabel('State')
    plt.ylabel('Frequency')
    plt.title('Frequency of States in Tweet Data')
    plt.show()
    
    ----------------------------------------------------------------------------------------
    3.Trend analysis : Analyze the time data associated with social media and analyse 
    its trends
    # Exp 3
    
    '''
    In this example, the program reads in a CSV file with social media data and converts the 'date' column to a datetime data type using pd.to_datetime(). The 'date' column is then set as the index of the DataFrame using set_index().
    
    Next, the data is resampled by month using the resample() method, and the number of entries in each month is counted using the count() method. The resulting monthly counts are plotted over time using matplotlib.
    
    You can modify the program to read in your own CSV file with time data associated with social media.
    '''
    
    import pandas as pd
    import matplotlib.pyplot as plt
    
    # Read in the CSV file
    df = pd.read_csv('trend_analysis.csv')
    
    # Convert the 'date' column to a datetime data type
    df['date'] = pd.to_datetime(df['date'])
    
    print(df.head())
    
    # Set the 'date' column as the index of the DataFrame
    df.set_index('date', inplace=True)
    
    # Resample the data by day and count the number of entries in each day
    # argument 'D' indicating that we want to resample by day.
    # daily_counts = df.resample('D').count()
    daily_counts = df.resample('D').count()
    print("Daily Counts: \n", daily_counts)
    
    
    # Plot the daily counts over time
    plt.plot(daily_counts.index, daily_counts['id'])
    plt.xlabel('Day')
    plt.ylabel('Number of Entries')
    plt.title('Social Media Trends')
    plt.show()
    ----------------------------------------------------------------------------
    4.Hashtag popularity analysis: Determine which hashtags are most popular among 
    different user groups
    # Exp 4
    
    '''
    To determine which hashtags are most popular among different user groups, we can use the following approach:
    
    - Read in the CSV file containing the social media data and filter it to include only the columns we need (e.g. user ID, user group, and hashtags).
    - Split the hashtags into individual words and count their frequency for each user group.
    - Plot the top N most frequent hashtags for each user group in a bar chart.
    '''
    
    import pandas as pd
    import matplotlib.pyplot as plt
    
    # Read in the CSV file
    df = pd.read_csv('hashtag_analysis.csv')
    
    # Group the data by hashtags and user groups, and count the occurrences
    hashtags_by_group = df.groupby(
        ['user_group', 'hashtags']).size().reset_index(name='count')
    
    # Plot a horizontal bar chart for each user group
    for group in df['user_group'].unique():
        group_data = hashtags_by_group[hashtags_by_group['user_group'] == group].sort_values('count', ascending=False).head(10)
        ax = group_data.plot(kind='barh', x='hashtags', y='count',
                             legend=False, title=f'Top Hashtags for {group}')
        ax.set_xlabel('Frequency')
        plt.tight_layout()
        plt.show()
    
    '''
    for i,row in dataset.iterrows():
        loc_data = ast.literal_eval(row['additional_data'])
        hashtags = [token for token in row.tweet.split() if token.startswith('#')]
        dataset['Hashtags'][i] = hashtags
    
        if loc_data['place']:
            dataset['Country'][i] = loc_data['place']['country']
    '''
    ----------------------------------------------------------------------------------------------
    5.Sentiment Analysis for given dataset
     Negative tweets analysis
     Positive tweets analysis
    # Exp 5
    '''
    This program first reads the dataset into a pandas DataFrame. Then, it uses TextBlob to perform sentiment analysis on each tweet and stores the polarity score in a new column called "polarity".
    
    Next, it creates separate DataFrames for positive and negative tweets using the "label" column. Finally, it plots the distribution of polarity scores for positive and negative tweets using seaborn and matplotlib. The resulting plot shows the density of polarity scores for each sentiment label, allowing us to compare the distribution of sentiment in the dataset.
    
    Note that this is a very basic example of sentiment analysis and there are many ways to improve it, such as using more advanced natural language processing techniques, handling negation and sarcasm, and using a larger and more diverse dataset.
    '''
    
    import pandas as pd
    from textblob import TextBlob
    import seaborn as sns
    import matplotlib.pyplot as plt
    
    # Read the dataset into a pandas DataFrame
    df = pd.read_csv("sentiment.csv")
    
    # Perform sentiment analysis using TextBlob and store the polarity score
    df["polarity"] = df["text"].apply(lambda x: TextBlob(x).sentiment.polarity)
    print(df.head())
    
    # Create separate DataFrames for positive and negative tweets
    pos_tweets = df[df["label"] == "positive"]
    neg_tweets = df[df["label"] == "negative"]
    
    # Plot the distribution of polarity scores for positive and negative tweets
    # A kernel density estimate (KDE) plot is a method for visualizing the distribution of observations in a dataset, analogous to a histogram.
    sns.kdeplot(pos_tweets["polarity"], shade=True, label="Positive")
    sns.kdeplot(neg_tweets["polarity"], shade=True, label="Negative")
    plt.xlabel("Polarity Score")
    plt.ylabel("Density")
    plt.title("Sentiment Analysis of Tweets")
    plt.legend()
    plt.show()
    -----------------------------------------------------------------------------------------------------------
    6.User engagement analysis: analyze how users engage with content on social 
    media to understand what types of content are most engaging
    
    # Exp 6
    '''
    To analyze how users engage with content on Twitter and understand what types of content are most engaging, we will use the following libraries:
    
    pandas: for reading and manipulating the dataset
    seaborn and matplotlib: for data visualization
    nltk: for text processing
    textblob: for sentiment analysis
    wordcloud: for creating a word cloud of the most common words in the dataset
    Assuming that the dataset is in a CSV format with a "text" column containing the tweet text, a "retweets" column containing the number of retweets, a "favorites" column containing the number of favorites, and a "date" column containing the date and time of the tweet, the program can be written as follows:
    '''
    
    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt
    import nltk
    from nltk.corpus import stopwords
    from textblob import TextBlob
    from wordcloud import WordCloud
    
    # Read the dataset into a pandas DataFrame
    df = pd.read_csv("engagement.csv")
    
    # Remove stop words from the tweet text
    # stop_words = set(stopwords.words("english"))
    # df["text"] = df["text"].apply(lambda x: " ".join(
        # word for word in x.split() if word.lower() not in stop_words))
    
    # Perform sentiment analysis using TextBlob and store the polarity score
    df["polarity"] = df["text"].apply(lambda x: TextBlob(x).sentiment.polarity)
    
    # Create a scatter plot of retweets vs. favorites to see how users engage with the content
    sns.scatterplot(x="retweets", y="favorites", data=df)
    plt.xlabel("Retweets")
    plt.ylabel("Favorites")
    plt.title("Engagement Analysis of Tweets")
    plt.show()
    
    # Create a bar chart of the most common words in the tweet text
    wordcloud = WordCloud(background_color="white", max_words=50,
                          contour_width=3, contour_color="steelblue")
    wordcloud.generate(" ".join(df["text"]))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.show()
    
    # Create a histogram of the polarity scores to see the distribution of sentiment in the dataset
    sns.histplot(data=df, x=df["polarity"], bins=20)
    plt.xlabel("Polarity Score")
    plt.ylabel("Count")
    plt.title("Sentiment Analysis of Tweets")
    plt.show()
    -------------------------------------------------------------------------------------------------
    7.Exploratory Data Analysis and visualization for given data set
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns

    # Load the dataset
    df = pd.read_csv('data.csv')

    # Explore the data
    print(df.head())
    print(df.info())
    print(df.describe())
    print(df.isnull().sum())

    # Preprocessing
    # Handle missing values
    # Example: df = df.dropna() or df.fillna(value)
    # For demonstration, let's fill missing numerical values with mean and categorical values with mode
    df['numerical_column'].fillna(df['numerical_column'].mean(), inplace=True)
    df['categorical_column'].fillna(df['categorical_column'].mode()[0], inplace=True)

    # Data Visualization
    plt.figure(figsize=(10, 6))
    sns.histplot(df['numerical_column'], kde=True)
    plt.title('Histogram of Numerical Column')
    plt.xlabel('Value')
    plt.ylabel('Frequency')
    plt.show()

    plt.figure(figsize=(10, 6))
    sns.boxplot(x='categorical_column', y='numerical_column', data=df)
    plt.title('Boxplot of Numerical Column by Category')
    plt.xlabel('Category')
    plt.ylabel('Numerical Column')
    plt.show()

    plt.figure(figsize=(10, 8))
    sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
    plt.title('Correlation Heatmap')
    plt.show()
    ----------------------------------------------------------------------------------------
    8. Brand analysis: analyze the conversation around a particular brand for given 
    dataset 
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from nltk.sentiment.vader import SentimentIntensityAnalyzer
    from sklearn.feature_extraction.text import TfidfVectorizer
    import nltk
    nltk.download('vader_lexicon')
    
    
    # Generate mock conversation data
    np.random.seed(0)
    num_samples = 1000
    brands = ['BrandX', 'BrandY', 'BrandZ']
    conversations = []
    
    for _ in range(num_samples):
        brand = np.random.choice(brands)
        if brand == 'BrandX':
            conversations.append("BrandX is the best! I love their products.")
        elif brand == 'BrandY':
            conversations.append("I had a bad experience with BrandY's customer service.")
        else:
            conversations.append("I'm thinking about trying out BrandZ's new product.")
    
    # Create DataFrame
    df = pd.DataFrame({'Brand': np.random.choice(brands, num_samples), 'Conversation': conversations})
    
    # Initialize Sentiment Intensity Analyzer
    sid = SentimentIntensityAnalyzer()
    
    # Perform sentiment analysis and add sentiment scores to DataFrame
    df['Sentiment'] = df['Conversation'].apply(lambda x: sid.polarity_scores(x)['compound'])
    
    # Initialize TF-IDF Vectorizer
    tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
    
    # Fit and transform the text data
    tfidf_matrix = tfidf_vectorizer.fit_transform(df['Conversation'])
    
    # Get feature names
    feature_names = tfidf_vectorizer.get_feature_names_out()
    
    # Display top words associated with each brand based on TF-IDF scores
    for brand in brands:
        print(f"\nTop words for {brand}:")
        indices = df.index[df['Brand'] == brand]
        top_words_indices = np.argsort(tfidf_matrix[indices].toarray().sum(axis=0))[::-1][:10]
        top_words = [feature_names[idx] for idx in top_words_indices]
        print(top_words)
    ------------------------------------------------------------------------

    9.Analyze competitor activities using social media for given problem statement
    import pandas as pd
    import matplotlib.pyplot as plt
    from nltk import RAKE
    import seaborn as sns

    # Read competitor data from CSV files
    competitor1_data = pd.read_csv('competitor1_data.csv')
    competitor2_data = pd.read_csv('competitor2_data.csv')

    # Extract keywords using NLTK RAKE
    def extract_keywords(text):
        rake = RAKE()
        return rake.extract_keywords_from_text(text)

    # Analyze competitors' social media activities
    def analyze_competitors(competitor_data):
        keywords = extract_keywords(' '.join(competitor_data['posts']))
        # Perform analysis and return relevant metrics

    # Collect and analyze data for each competitor
    competitor1_metrics = analyze_competitors(competitor1_data)
    competitor2_metrics = analyze_competitors(competitor2_data)

    # Create DataFrame for comparative analysis
    df = pd.DataFrame({'Competitor 1': competitor1_metrics, 'Competitor 2': competitor2_metrics})

    # Plot results
    plt.figure(figsize=(10, 6))
    sns.barplot(data=df)
    plt.title('Comparative Analysis of Competitors')
    plt.xlabel('Metrics')
    plt.ylabel('Values')
    plt.xticks(rotation=45)
    plt.show()
    ------------------------------------------------------------------------
    
    10.Social Network data analysis for community detection and influential 
    analytics for given problem using Girvan newman algorithm / Kmean clustering 
    algorith
    import pandas as pd
    import warnings
    import networkx as nx
    from networkx.algorithms import community
    import matplotlib.pyplot as plt
    warnings.filterwarnings('ignore')
    
    dataset = pd.read_csv('tweets.csv')
    dataset = dataset[dataset.user_location.notnull()][:10]
    dataset['hashtags'] = ''
    
    for i, row in dataset.iterrows():
        hashtags = [token for token in row.tweet.split() if token.startswith('#')]
        dataset['hashtags'][i] = hashtags
    
    dataset = dataset.explode('hashtags')
    users = list(dataset['user'].unique())
    hashtags = list(dataset['hashtags'].unique())
    
    vis = nx.Graph()
    vis.add_nodes_from(users + hashtags)
    
    for name, group in dataset.groupby(['hashtags', 'user']):
        hashtag, user = name
        weight = len(group)
        vis.add_edge(hashtag, user, weight=weight)
    
    
    # Community Detection
    community_gen = community.girvan_newman(vis)
    top_level_communities = next(community_gen)
    
    communities = list(next(community_gen))
    print("Comm", communities)
    # colors = ['red', 'yellow', 'orange']
    # node_colors = {}
    
    # for i, comm in enumerate(communities):
    #     for node in comm:
    #         node_colors[node] = colors[i]
    
    # nx.set_node_attributes(vis, node_colors, name='color')
    # node_colors = nx.get_node_attributes(vis, 'color')
    
    nx.draw(vis)
    plt.axis('off')
    plt.show()
    
    
    # Influential Node Analysis
    centrality = nx.betweenness_centrality(vis)
    print('Top 5 Most Influential Users/Hashtags based on betweenness centrality :- ')
    influential_nodes = sorted(
        centrality.items(), key=lambda item: item[1], reverse=True)[:5]
    
    for nodes in influential_nodes:
        print(f'{nodes[0]} : {nodes[1]}')
</pre>
</body>

</html>
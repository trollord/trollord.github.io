<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>HAMMING</title>
</head>

<body>
	<pre>

#1 Imputation 
import pandas as pd
from sklearn.impute import SimpleImputer
from scipy import stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder

# # Create the DataFrame
# data = {
#  'Student ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
#  'Exam 1 Score': [85, 76, 90, 65, 88, None, 78, 92, 85, 70],
#  'Exam 2 Score': [92, 78, 88, 75, 91, 82, 76, 96, 89, 68],
#  'Exam 3 Score': [88, None, 94, 80, 87, 79, 72, 98, 91, 75],
#  'Final Grade': ['A', 'B', 'A', 'C', 'A', 'B', 'C', 'A', 'A', 'B']
# }
# df = pd.DataFrame(data)
# Save to CSV
# df.to_csv('your_dataset.csv', index=False)

# Import the DataFrame
df = pd.read_csv('your_dataset.csv')

print("Dataset:\n", df)
print("\n\n\n\n")

# Imputation
df1 = df.copy()  # Create a copy to avoid modifying the original DataFrame
imputer = SimpleImputer(strategy="mean")
df1[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']] = imputer.fit_transform(df1[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']])
print("Imputation:\n", df1)
print("\n\n\n\n")

# Anomaly Detection
z_scores = stats.zscore(df1[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']])
threshold = 3
outliers = (abs(z_scores) > threshold).any(axis=1)
df1['Is Outlier'] = outliers
print("Anomaly Detection:\n", df1)
print("\n\n\n\n")

# Standardization
scaler = StandardScaler()
df1[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']] = scaler.fit_transform(df1[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']])
print("Standardization:\n", df1)
print("\n\n\n\n")

# Normalization
scaler = MinMaxScaler()
df1[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']] = scaler.fit_transform(df1[['Exam 1 Score', 'Exam 2 Score', 'Exam 3 Score']])
print("Normalization:\n", df1)
print("\n\n\n\n")

# Encoding (if applicable)
# Assuming 'Categorical_Column' is the name of the column you want to encode
if 'Exam 1 Score' in df1:
	encoder = OneHotEncoder()
	encoded_data = encoder.fit_transform(df1[['Exam 1 Score']].values.reshape(-1, 1)).toarray()
	encoded_columns = encoder.get_feature_names_out(['Exam 1 Score'])
	encoded_df = pd.DataFrame(encoded_data, columns=encoded_columns)
	df1 = pd.concat([df1, encoded_df], axis=1)
	df1.drop(['Exam 1 Score'], axis=1, inplace=True)

print("Encoding:\n", df1)
print("\n\n\n\n")

# Output
print("Final Preprocessed Dataset:\n", df1)


#2.Decent
import numpy as np
import math
import matplotlib.pyplot as plt

# Define the objective function you want to minimize.
def objective_function(x):
	# This is a simple quadratic function (a parabola).
	# f(x) = (x-2)^2 + (x-0)^2
	return (x - 2)**2 + (x - 0)**2

# Define the gradient of the objective function.
def gradient(x):
	# The gradient represents the slope of the function at a given point.
	# For this function, the gradient is 2 times the input 'x'.
	return 2 * x

# Set the learning rate, which determines the step size in each iteration.
learning_rate = 0.1

# Set an initial value for 'x', where the optimization process will start.
initial_x = 5

# Specify the number of iterations to perform during the gradient descent process.
num_iterations = 200

# Initialize 'x' with the initial value.
x = initial_x

# Create lists to keep track of the history of 'x' and the value of the objective function during optimization.
x_history = [x]
loss_history = [objective_function(x)]

# Perform gradient descent for a specified number of iterations.
for i in range(num_iterations):
	# Compute the gradient of the objective function at the current 'x'.
	grad = gradient(x)

	# Update 'x' by moving it in the direction of steepest descent (opposite to the gradient) with a step determined by the learning rate.
	x -= learning_rate * grad

	# Append the updated 'x' and the value of the objective function at the new 'x' to their respective histories.
	x_history.append(x)
	loss_history.append(objective_function(x))

# Create a range of 'x' values to plot the objective function.
x_values = np.linspace(-6, 6, 400)

# Compute the corresponding values of the objective function.
y_values = objective_function(x_values)

# Plot the objective function and the history of 'x' values during optimization.
plt.plot(x_values, y_values, label='Objective Function')
plt.scatter(x_history, loss_history, color='red', label='Gradient Descent Steps')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Gradient Descent Optimization')
plt.legend()

# Display the plot.
plt.show()


#3.LinearRegression
# Simple Linear Regression without using SKLearn (Equation Given)

import numpy as np
import matplotlib.pyplot as plt

# Generate sample data
np.random.seed(42)
X = np.random.rand(100, 1)
y = 3 * X + 2 + 0.1 * np.random.randn(100, 1)  # y = 3X + 2 + noise

# Calculate mean of X and y
x_mean = np.mean(X)
print("Mean of X : ", x_mean)
y_mean = np.mean(y)
print("Mean of Y : ", y_mean)

# Calculate slope (m) and intercept (b) using closed-form formulas
numerator = np.sum((X - x_mean) * (y - y_mean))
denominator = np.sum((X - x_mean) ** 2)
slope = numerator / denominator
intercept = y_mean - slope * x_mean

print("The slope and Intercept are:-")
print("Slope:", slope)
print("Intercept:", intercept)

plt.scatter(X, y, label='Data Points')
plt.plot(X, slope * X + intercept, color='red', label='Regression Line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Simple Linear Regression')
plt.legend()
plt.show()




# Simple Linear Regression without using SKLearn(Data Given not equation)

import matplotlib.pyplot as plt
import numpy as np
import math


x = [151, 174, 138, 186, 128, 136, 179, 163, 152, 131]
y = [63, 81, 56, 91, 47, 57, 76, 72, 62, 48]

# plt.scatter(x, y)
# plt.show()

mean_x = np.mean(x)
mean_y = np.mean(y)
print(mean_x, "," , mean_y)

l=len(x)
numerator=0
denominator=0
for i in range(l):
	numerator += (x[i] - mean_x) * (y[i] - mean_y)
	denominator += (x[i] - mean_x) **2
m = numerator / denominator
c = mean_y - (m * mean_x)
print(m,c)


max_x = np.max(x) + 100
min_x = np.min(y) - 100
X = np.linspace(min_x,max_x,100)
Y = c + m*X

plt.plot(X, Y, color='green', label = 'Regression Line')
plt.scatter(x, y, c = 'black', label = 'data')
plt.xlabel('Height')
plt.ylabel('Weight')
plt.legend()
plt.show()

#4.Logistic Regression
import numpy as np

class LogisticRegression:
	def __init__(self, learning_rate=0.01, num_iterations=1000):
		# Initialize the logistic regression model with learning rate and number of iterations
		self.learning_rate = learning_rate
		self.num_iterations = num_iterations
		self.weights = None
		self.bias = None

	def sigmoid(self, z):
		# Define the sigmoid function used for logistic regression
		return 1 / (1 + np.exp(-z))

	def fit(self, X, y):
		# Train the logistic regression model with input data X and labels y
		num_samples, num_features = X.shape

		# Initialize weights and bias
		self.weights = np.zeros(num_features)
		self.bias = 0

		# Iteratively update model parameters
		for _ in range(self.num_iterations):
			linear_model = np.dot(X, self.weights) + self.bias
			predictions = self.sigmoid(linear_model)

			# Calculate gradients for weights and bias
			dw = (1 / num_samples) * np.dot(X.T, (predictions - y))
			db = (1 / num_samples) * np.sum(predictions - y)

			# Update model parameters using gradients and learning rate
			self.weights -= self.learning_rate * dw
			self.bias -= self.learning_rate * db

	def predict(self, T):
		# Make predictions on new data T
		linear_model = np.dot(T, self.weights) + self.bias
		predictions = self.sigmoid(linear_model)

		# Convert predicted probabilities to binary labels (0 or 1)
		predicted_labels = [1 if p >= 0.5 else 0 for p in predictions]

		return predicted_labels

# Sample data
X = np.array([[2.5, 1.5], [3.0, 1.0], [4.0, 3.0], [1.0, 4.0], [2.0, 2.0]])
print("X: ", X)
Y = np.array([1, 1, 0, 0, 1])
print("Y: ", Y)

T = np.array([[2, 1], [3.0, 1.5], [4.0, 3.0], [1.0, 4.0], [2.5, 2.0]])

# Create a logistic regression model with specified parameters
model = LogisticRegression()

# Train the model on the sample data
model.fit(X, Y)

# Make predictions on new data
y_pred = model.predict(T)

# Print the learned weights and bias, as well as the predicted labels
print("Learned Weights:", model.weights)
print("Learned Bias:", model.bias)
print("Predicted Labels:", y_pred)

#5.Decision TRee
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load the Breast Cancer dataset
data = load_breast_cancer()
X = data.data
y = data.target
print(X)
print(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Decision Tree classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the classifier on the training data
clf.fit(X_train, y_train)

# Make predictions on the test data
y_pred = clf.predict(X_test)

# Calculate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)
accuracy=accuracy*100
print(f"Accuracy: {accuracy:.2f}%")

#6.SVM
# Import necessary libraries
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the Breast Cancer dataset
cancer = datasets.load_breast_cancer()
X = cancer.data  # Features
y = cancer.target  # Target labels

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create an SVM classifier with a linear kernel and regularization parameter (C) set to 1.0
svm_classifier = SVC(kernel='linear', C=1.0)

# Train the SVM model on the training data
svm_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = svm_classifier.predict(X_test)

# Calculate and print the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Print a classification report and a confusion matrix for more detailed evaluation
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

#7.PCA 
# Import necessary libraries
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load the Breast Cancer dataset
cancer = datasets.load_breast_cancer()
X = cancer.data  # Features
y = cancer.target  # Target labels

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create an SVM classifier with a linear kernel and regularization parameter (C) set to 1.0
svm_classifier = SVC(kernel='linear', C=1.0)

# Train the SVM model on the training data
svm_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = svm_classifier.predict(X_test)

# Calculate and print the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Print a classification report and a confusion matrix for more detailed evaluation
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

8.Ensemble
from sklearn.ensemble import StackingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Breast Cancer Wisconsin dataset
data = load_breast_cancer()
X, y = data.data, data.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define base estimators
base_estimators = [
	('decision_tree', DecisionTreeClassifier()),
	('random_forest', RandomForestClassifier()),
	('svm', SVC())
]

# Create a StackingClassifier with a meta-learner (e.g., Decision Tree Classifier)
stacking_classifier = StackingClassifier(estimators=base_estimators, final_estimator=DecisionTreeClassifier(), cv=5)

# Fit the ensemble model on the training data
stacking_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = stacking_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
accuracy = accuracy*100
print(f"Accuracy: {accuracy:}")
        
        
        

	</pre>
</body>

</html>